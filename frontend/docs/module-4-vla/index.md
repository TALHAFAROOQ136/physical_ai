---
sidebar_position: 1
title: Introduction to VLA
description: Vision-Language-Action models for robot control
---

# Module 4: VLA - Vision-Language-Action

Vision-Language-Action (VLA) models combine visual perception, language understanding, and robot control into unified AI systems.

## Learning Objectives

By the end of this module, you will be able to:

- Implement voice interfaces for robot control
- Integrate LLMs for task planning
- Build end-to-end VLA pipelines
- Create natural language robot interactions

## What is VLA?

VLA models enable robots to:

- **See** - Process visual input from cameras
- **Understand** - Interpret natural language commands
- **Act** - Generate robot actions from multimodal input

## Module Structure

1. **[Voice Interfaces](/docs/module-4-vla/voice-interfaces)** - Speech recognition and synthesis
2. **[LLM Planning](/docs/module-4-vla/llm-planning)** - Task planning with language models

## Key Technologies

| Technology | Purpose |
|------------|---------|
| **Whisper** | Speech-to-text |
| **GPT-4** | Language understanding |
| **RT-2** | Vision-language-action |
| **CLIP** | Vision-language alignment |

## Prerequisites

- Completed [Module 3: NVIDIA Isaac](/docs/module-3-isaac)
- Understanding of deep learning basics
- API access to OpenAI or similar LLM provider

---

Begin with [Voice Interfaces](/docs/module-4-vla/voice-interfaces)!
